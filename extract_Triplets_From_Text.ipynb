{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Prends en entr√©e un dossier contenant les fichiers textes et ressort des fichiers textes contenant les triplets\n"
      ],
      "metadata": {
        "id": "Qc7sQNpNa6A2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "test enrichissement\n"
      ],
      "metadata": {
        "id": "j3ej9PuQ6g0V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-tB42GDNWs1",
        "outputId": "26262a08-e9a4-449a-d2bb-ca404bd1e58b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed page_18.txt and saved results to /content/output_triplets/triplets_page_18.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed page_8.txt and saved results to /content/output_triplets/triplets_page_8.txt\n",
            "Processed page_16.txt and saved results to /content/output_triplets/triplets_page_16.txt\n",
            "Processed page_22.txt and saved results to /content/output_triplets/triplets_page_22.txt\n",
            "Processed page_15.txt and saved results to /content/output_triplets/triplets_page_15.txt\n",
            "Processed page_11.txt and saved results to /content/output_triplets/triplets_page_11.txt\n",
            "Processed page_13.txt and saved results to /content/output_triplets/triplets_page_13.txt\n",
            "Processed page_20.txt and saved results to /content/output_triplets/triplets_page_20.txt\n",
            "Processed page_10.txt and saved results to /content/output_triplets/triplets_page_10.txt\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Initialize REBEL pipeline\n",
        "triplet_extractor = pipeline('text2text-generation',\n",
        "                             model='Babelscape/rebel-large',\n",
        "                             tokenizer='Babelscape/rebel-large')\n",
        "\n",
        "# Relation mapping dictionary\n",
        "relation_mapping = {\n",
        "    \"participated in\": [\"competed in\", \"participant in\", \"sports discipline competed in\"],\n",
        "    \"sport\": [\"sports discipline\", \"field of work\"],\n",
        "    \"located in\": [\"located in or next to body of water\", \"located in the administrative territorial entity\"],\n",
        "    \"point in time\": [\"follows\", \"followed by\", \"point in time\"],\n",
        "    \"instance of\": [\"is a list of\", \"instance of\"],\n",
        "}\n",
        "\n",
        "# Function to chunk text into smaller parts\n",
        "def chunk_text_by_sentences(text, sentences_per_chunk=3):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "    chunks = [' '.join(sentences[i:i + sentences_per_chunk]) for i in range(0, len(sentences), sentences_per_chunk)]\n",
        "    return chunks\n",
        "\n",
        "# Function to extract triplets from the output text\n",
        "def extract_triplets(text):\n",
        "    triplets = []\n",
        "    relation, subject, object_ = '', '', ''\n",
        "    current = 'x'\n",
        "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
        "        if token == \"<triplet>\":\n",
        "            current = 't'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
        "                relation = ''\n",
        "            subject = ''\n",
        "        elif token == \"<subj>\":\n",
        "            current = 's'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
        "            object_ = ''\n",
        "        elif token == \"<obj>\":\n",
        "            current = 'o'\n",
        "            relation = ''\n",
        "        else:\n",
        "            if current == 't':\n",
        "                subject += ' ' + token\n",
        "            elif current == 's':\n",
        "                object_ += ' ' + token\n",
        "            elif current == 'o':\n",
        "                relation += ' ' + token\n",
        "    if subject != '' and relation != '' and object_ != '':\n",
        "        triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
        "    return triplets\n",
        "\n",
        "# Function to remap relations\n",
        "def remap_relations(triplets, mapping):\n",
        "    for triplet in triplets:\n",
        "        for normalized_relation, synonyms in mapping.items():\n",
        "            if triplet['type'] in synonyms:\n",
        "                triplet['type'] = normalized_relation\n",
        "                break\n",
        "    return triplets\n",
        "\n",
        "# Function to process text and extract triplets\n",
        "def process_text(text, sentences_per_chunk=1, num_beams=10, max_length=256):\n",
        "    text_chunks = chunk_text_by_sentences(text, sentences_per_chunk=sentences_per_chunk)\n",
        "    all_triplets = []\n",
        "    for chunk in text_chunks:\n",
        "        extracted_text = triplet_extractor.tokenizer.batch_decode([\n",
        "            triplet_extractor(chunk,\n",
        "                              return_tensors=True,\n",
        "                              return_text=False,\n",
        "                              num_beams=num_beams,\n",
        "                              early_stopping=False,\n",
        "                              max_length=max_length)[0][\"generated_token_ids\"]\n",
        "        ])\n",
        "        chunk_triplets = extract_triplets(extracted_text[0])\n",
        "        all_triplets.extend(chunk_triplets)\n",
        "    all_triplets = remap_relations(all_triplets, relation_mapping)\n",
        "    return all_triplets\n",
        "\n",
        "# Remove duplicate triplets\n",
        "def remove_duplicate_triplets(triplets):\n",
        "    unique_triplets = {frozenset(triplet.items()): triplet for triplet in triplets}\n",
        "    return list(unique_triplets.values())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_folder = os.path.join(os.getcwd(), \"modified_text\")\n",
        "    output_folder = os.path.join(os.getcwd(), \"output_triplets\")\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(input_folder):\n",
        "        print(f\"Input folder not found: {input_folder}\")\n",
        "    else:\n",
        "        for file_name in os.listdir(input_folder):\n",
        "            if file_name.endswith(\".txt\"):\n",
        "                input_path = os.path.join(input_folder, file_name)\n",
        "                output_path = os.path.join(output_folder, f\"triplets_{file_name}\")\n",
        "\n",
        "                with open(input_path, 'r', encoding='utf-8') as file:\n",
        "                    text = file.read().strip()\n",
        "\n",
        "                triplets = process_text(text)\n",
        "                unique_triplets = remove_duplicate_triplets(triplets)\n",
        "\n",
        "                with open(output_path, 'w', encoding='utf-8') as output_file:\n",
        "                    json.dump(unique_triplets, output_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "                print(f\"Processed {file_name} and saved results to {output_path}\")\n"
      ]
    }
  ]
}